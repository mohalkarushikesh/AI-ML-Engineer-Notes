
**Artificial Intelligence (AI)**  
- The broad field focused on creating machines that can perform tasks requiring human-like intelligence‚Äîsuch as reasoning, learning, and problem-solving.

**Machine Learning (ML)**  
- A subset of AI that enables systems to learn from data and improve performance over time without being explicitly programmed.

**Deep Learning**  
- A specialized branch of ML that uses neural networks with multiple layers to learn complex patterns, especially in large datasets like images, audio, or text.

---


**List**  
- Ordered, mutable collection of items.  
- Defined with square brackets: `[]`  
- Allows duplicates.  
- Example: `my_list = [1, 2, 3]`

---

**Dictionary**  
- Unordered, mutable collection of key-value pairs.  
- Defined with curly braces and colons: `{}`  
- Keys must be unique.  
- Example: `my_dict = {'a': 1, 'b': 2}`

---

**Set**  
- Unordered, mutable collection of unique items.  
- Defined with curly braces: `{}`  
- No duplicates allowed.  
- Example: `my_set = {1, 2, 3}`

---

**Tuple**  
- Ordered, immutable collection of items.  
- Defined with parentheses: `()`  
- Allows duplicates.  
- Example: `my_tuple = (1, 2, 3)`

---

NumPy
- Handles arrays and numerical operations efficiently.
-	Example: vector math, matrix manipulation.
Pandas
-	Manages tabular data using DataFrames.
-	Ideal for data cleaning and analysis.
Matplotlib
-	Basic plotting library for charts and graphs.
-	Great for line plots, bar charts, histograms.
Seaborn
-	Built on Matplotlib; adds prettier, statistical plots.
-	Example: heatmaps, violin plots, regression plots.
Scikit-learn
-	Core ML library for models and evaluation.
-	Includes regression, classification, clustering, and more.
Deep Learning Libraries
-	TensorFlow ‚Äì Google's framework for building and training neural networks.
-	Keras ‚Äì User-friendly wrapper around TensorFlow for fast prototyping.
-	PyTorch ‚Äì Facebook‚Äôs flexible deep learning library with dynamic graphs.

üìö NLTK (Natural Language Toolkit)
- A classic library for working with human language data.
- Supports tokenization, stemming, tagging, parsing, and corpus access.
-	Great for educational and research use.

üó£Ô∏è spaCy
- Industrial-strength NLP library built for speed and efficiency.
-	Handles tagging, parsing, named entity recognition (NER), and more.
-	Comes with pre-trained models for many languages.

üß† TextBlob
-	Simple NLP tool built on NLTK and Pattern.
- Easy interface for sentiment analysis, translation, and more.

üí¨ Gensim
- Specializes in topic modeling and document similarity.
- Widely used for word embeddings like Word2Vec.

üî† Transformers (by Hugging Face)
- Deep learning library focused on state-of-the-art models (BERT, GPT, etc.).
- Handles text classification, translation, summarization, Q&A, and more.

üßÆ Tesseract
- Optical Character Recognition (OCR) engine.
- Useful for extracting text from images or scanned documents.

üìä Beautiful Soup & Scrapy
- Not NLP libraries per se, but great for collecting text data from websites via web scraping.

---

- **Linear Algebra**:

**Scalar**  
- A single numerical value (just a number).  
- Example: `5`, `-3.14`

**Vector**  
- An ordered list of numbers representing direction and magnitude.  
- Example: `[2, -1, 3]` ‚Äî often visualized as an arrow in space.

**Matrix**  
- A rectangular grid of numbers arranged in rows and columns.  
- Example: `[[1, 2], [3, 4]]` ‚Äî used for linear transformations and storing data.

- **Calculus**: 
- Function: A rule that maps inputs to outputs.
- Derivative: Measures the rate of change of a function.
- Gradient: A vector showing direction and rate of steepest increase.

- **Statistics**:
________________________________________
- Mean ‚Äì Average value of a dataset.
- Median ‚Äì Middle value when data is sorted.
- Variance ‚Äì Measures spread of data from the mean.
- Standard Deviation ‚Äì Square root of variance; shows how much data varies.
________________________________________
- Basic Probability ‚Äì Likelihood of an event occurring, between 0 and 1.
________________________________________
**Supervised Learning**

*Regression*  
- **Linear Regression** ‚Äì Models the relationship between dependent and independent variables using a straight line.  
- **Polynomial Regression** ‚Äì Extends linear regression by fitting a polynomial equation.  
- **Support Vector Regression (SVR)** ‚Äì Uses support vectors to find a best-fit hyperplane for regression problems.  
- **Decision Tree Regression** ‚Äì Splits data into branches to model continuous outputs.  
- **Random Forest Regression** ‚Äì A collection of decision trees that improve predictive accuracy.  
- **Gradient Boosting Regression** ‚Äì Sequentially builds models that correct previous errors.

*Classification*  
- **Logistic Regression** ‚Äì Predicts binary outcomes using a logistic function.  
- **Support Vector Machines (SVM)** ‚Äì Finds the hyperplane that best separates data into classes.  
- **Decision Trees** ‚Äì Tree-like models for splitting data based on features.  
- **Random Forest** ‚Äì Combines multiple decision trees to reduce variance.  
- **Naive Bayes** ‚Äì Probabilistic model based on Bayes‚Äô theorem with strong independence assumptions.  
- **K-Nearest Neighbors (KNN)** ‚Äì Classifies data points based on their nearest neighbors.  
- **Neural Networks** ‚Äì Computational models that mimic human brain function for pattern recognition.  
- **Gradient Boosting** ‚Äì Builds classifiers sequentially to minimize error.  
- **Linear Discriminant Analysis** ‚Äì Projects data to maximize class separability.  
- **XGBoost** ‚Äì An efficient, scalable implementation of gradient boosting.

---

**Unsupervised Learning**

*Clustering*  
- **K-Means Clustering** ‚Äì Divides data into K clusters by minimizing intra-cluster variance.  
- **Hierarchical Clustering** ‚Äì Builds nested clusters by merging or splitting groups.  
- **DBSCAN** ‚Äì Density-based clustering that groups closely packed points.  
- **Gaussian Mixture Models** ‚Äì Probabilistic model representing data as mixtures of Gaussians.

*Dimensionality Reduction*  
- **Principal Component Analysis (PCA)** ‚Äì Transforms data to lower dimensions while preserving variance.  
- **Linear Discriminant Analysis (LDA)** ‚Äì Used for class-based dimension reduction.  
- **t-SNE** ‚Äì Visualizes high-dimensional data in lower dimensions by preserving local structure.  
- **Independent Component Analysis (ICA)** ‚Äì Separates a multivariate signal into independent non-Gaussian signals.  
- **UMAP** ‚Äì Preserves both local and global data structure during reduction.

*Association Rule Learning*  
- **Apriori** ‚Äì Identifies frequent itemsets in transactional data and derives rules.

---

**Reinforcement Learning**

- **Q-learning** ‚Äì Learns optimal policies by maximizing expected rewards.  
- **SARSA** ‚Äì Similar to Q-learning but updates based on the action actually taken.

---

**Ensemble Methods**

Combine multiple models to improve predictions:  
- Random Forest  
- Gradient Boosting  
- Adaboost  
- Bagging  
- Stacking  
________________________________________
üìä Data Preprocessing
‚Ä¢	Feature Scaling ‚Äì Adjusts numeric features to a common scale.
o	Standardization: Rescales to mean 0, std. dev. 1.
o	Normalization: Scales values to a fixed range (like 0‚Äì1).
‚Ä¢	Encoding Categorical Variables
o	One-hot encoding: Turns categories into binary columns.
o	Label encoding: Assigns a unique number to each category.
‚Ä¢	Handling Imbalanced Data
o	SMOTE: Creates synthetic examples of minority class.
________________________________________
üß™ Model Evaluation:

  ‚Ä¢	Metrics
  o	MSE: Average of squared errors.
  o	RMSE: Square root of MSE.
  o	Accuracy: Correct predictions / total.
  o	Precision: True positives / predict
  ed positives.
  o	Recall: True positives / actual positives.
  o	F1-score: Balance between precision & recall.
  ‚Ä¢	Validation Techniques
  o	Train-test split: Separates data for training and testing.
  o	K-fold cross-validation: Repeated splitting for stable results.
  ‚Ä¢	Overfitting vs. Underfitting
  o	Overfitting: Model memorizes training data, poor on new data.
  o	Underfitting: Model too simple to capture data patterns.
________________________________________
üìê Mathematics (Intermediate):
  ‚Ä¢	Linear Algebra
  o	Matrix decomposition: Breaks matrix into simpler forms.
  o	Dot product: Combines two vectors to get a scalar.
  ‚Ä¢	Calculus
  o	Gradient descent: Optimizes by moving toward lowest error.
  o	Partial derivatives: Rate of change with respect to one variable.
  ‚Ä¢	Probability
  o	Conditional probability: Probability given some condition.
  o	Bayes‚Äô theorem: Updates probability based on new evidence.
________________________________________

**Neural Networks & Deep Learning**

*A broad category including specialized architectures and techniques.*

*Activation Functions*  
- **Sigmoid** ‚Äì Outputs between 0 and 1, good for binary classification.  
- **Tanh** ‚Äì Outputs between -1 and 1, centered around zero.  
- **Softmax** ‚Äì Converts values into a probability distribution.  
- **ReLU** ‚Äì Returns positive values or zero.  
- **Leaky ReLU** ‚Äì Allows small negative slope for negative inputs.  
- **ELU** ‚Äì Like ReLU, but smooth for negative inputs.  
- **SELU** ‚Äì Self-normalizing activation for deep networks.  
- **Swish** ‚Äì A smooth function: \( f(x) = x \cdot \text{sigmoid}(x) \)  
- **GELU** ‚Äì Approximates ReLU using Gaussian error function.

*Basic Architectures*  
- **Feedforward Neural Networks (FNN)** ‚Äì Data flows forward through layers.  
- **Multilayer Perceptron (MLP)** ‚Äì FNN with multiple hidden layers.  
- **Perceptron** ‚Äì The simplest model with a single layer and binary output.

*Specialized Architectures*  
- **Convolutional Neural Networks (CNN)** ‚Äì Ideal for image data using filters.  
- **Recurrent Neural Networks (RNN)** ‚Äì Designed for sequences and temporal patterns.  
- **Long Short-Term Memory Networks (LSTM)** ‚Äì RNN variant that captures long-term dependencies.  
- **Radial Basis Function Networks (RBF)** ‚Äì Uses radial functions for classification.  
- **Generative Adversarial Networks (GANs)** ‚Äì Two networks compete to generate realistic data.  
- **Autoencoders** ‚Äì Learn efficient codings by encoding and decoding input data.  
- **Modular Neural Networks** ‚Äì Composed of independent subnetworks.  
- **Sequence-to-Sequence Models** ‚Äì Maps input sequences to output sequences, great for translation.

*Other Concepts*  
- **Deep Learning** ‚Äì Multiple-layer networks for learning hierarchical features.  
- **Graph Neural Networks** ‚Äì Work with graph-structured data.  
- **Quantum Neural Networks** ‚Äì Combine quantum computing with neural networks.

---

There we go‚Äîeverything in one detailed yet readable format. Want this turned into flashcards or visual mind maps? I‚Äôm totally game for that too.
