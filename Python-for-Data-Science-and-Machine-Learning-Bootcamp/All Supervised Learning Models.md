Hereâ€™s your **ultra-short Machine Learning (ML) algorithms cheatsheet** ğŸš€:

### **Supervised Learning**
1ï¸âƒ£ **Linear Regression** â†’ Predicts continuous values.Â  
 Â  _Ex:_ House price prediction.Â  


2ï¸âƒ£ **Logistic Regression** â†’ Predicts probabilities for classification.Â  
 Â  _Ex:_ Spam detection.Â  


3ï¸âƒ£ **Decision Tree** â†’ Splits data into rules for classification.Â  
 Â  _Ex:_ Loan approval system.Â  


4ï¸âƒ£ **Random Forest** â†’ Multiple decision trees for better predictions.Â  
 Â  _Ex:_ Fraud detection.Â  


5ï¸âƒ£ **Support Vector Machine (SVM)** â†’ Finds optimal boundary for separation.Â  
 Â  _Ex:_ Cancer cell classification.Â  


6ï¸âƒ£ **NaÃ¯ve Bayes** â†’ Probabilistic classification using Bayesâ€™ theorem.Â  
 Â  _Ex:_ Sentiment analysis.Â  


7ï¸âƒ£ **K-Nearest Neighbor (KNN)** â†’ Classifies based on closest neighbors.Â  
 Â  _Ex:_ Handwritten digit recognition.Â  


### **Unsupervised Learning**
8ï¸âƒ£ **K-Means Clustering** â†’ Groups similar data points into clusters.Â  
 Â  _Ex:_ Customer segmentation.Â  


9ï¸âƒ£ **Principal Component Analysis (PCA)** â†’ Reduces dataset size while keeping variance.Â  
 Â  _Ex:_ Face recognition.Â  


### **Deep Learning**
ğŸ”Ÿ **Neural Networks** â†’ Layers of artificial neurons for complex patterns.Â  
 Â  _Ex:_ Image recognition & NLP.Â  

---

### **Linear Regression**
- **Predicts continuous values** by fitting a straight line to the data.
- It assumes a **linear relationship** between independent (\(X\)) and dependent (\(Y\)) variables.

#### **ğŸ“Œ Equation**

$$Y = \beta_0 + \beta_1X + \epsilon$$

Where:
- \(Y\) â†’ Dependent variable (target)
- \(X\) â†’ Independent variable (input)
- \(\beta_0, \beta_1\) â†’ Model parameters (intercept & slope)
- \(\epsilon\) â†’ Error term (random noise)

---

### **ğŸ”¹ Use Case**
Used for predicting **continuous numerical values**.
- **Example**: Predicting **house prices** based on **area** and **number of rooms**.

---

### **ğŸ”¹ Types of Linear Regression**
1. **Simple Linear Regression** â†’ Uses **one independent variable**.
   - Example: Predicting **salary** based on **years of experience**.
2. **Multiple Linear Regression** â†’ Uses **two or more independent variables**.
   - Example: Predicting **house prices** based on **size, location, and number of bedrooms**.
3. **Polynomial Regression** â†’ Extends linear regression by adding polynomial terms to capture **non-linearity**.
   - Example: Modeling **growth rate** of bacteria over time.
4. **Ridge Regression (L2 Regularization)** â†’ Helps handle **multicollinearity** by shrinking large coefficients.
   - Example: Predicting **stock prices** with multiple correlated financial factors.
5. **Lasso Regression (L1 Regularization)** â†’ Shrinks some coefficients to **zero**, helping in **feature selection**.
   - Example: Selecting **key factors** affecting **customer satisfaction**.
6. **Elastic Net Regression** â†’ Combination of **Ridge & Lasso**, balancing feature selection & coefficient shrinkage.
   - Example: **Predicting sales revenue** with multiple interrelated factors.
7. **Stepwise Regression** â†’ Adds/removes predictors systematically based on significance.
   - Example: **Medical diagnosis models** where only important patient data is selected.
8. **Quantile Regression** â†’ Estimates percentiles instead of mean values.
   - Example: **House price estimation for different market segments**.
9. **Bayesian Regression** â†’ Uses probability distributions to estimate coefficients.
   - Example: **Weather forecasting with uncertain data**.
---

### **ğŸ”¹ Evaluation Metrics**
1. **Mean Absolute Error (MAE)** â†’ Measures average absolute difference between actual and predicted values.
2. **Mean Squared Error (MSE)** â†’ Penalizes larger errors more heavily.
3. **Root Mean Squared Error (RMSE)** â†’ The square root of MSE; expressed in the same units as the target variable.

---

### **ğŸ”¹ Example**
**Predicting house prices**  
- **Independent Variables (X)** â†’ Square footage, number of rooms  
- **Dependent Variable (Y)** â†’ Price  
- The best-fit regression line estimates house prices based on **historical data trends**.

---

### **Logistic Regression**
- **Predicts probabilities** for categorical outcomes (classification task).
- Unlike Linear Regression, it models data using the **logistic (sigmoid) function** to restrict predictions between 0 and 1.

#### **ğŸ“Œ Equation**

$$p(Y=1) = \frac{1}{1+e^{-(\beta_0 + \beta_1X)}}$$

Where:
- \(p(Y=1)\) â†’ Probability that the outcome belongs to class 1
- \(\beta_0, \beta_1\) â†’ Model parameters
- \(X\) â†’ Independent variable
- \(e\) â†’ Eulerâ€™s number (â‰ˆ 2.718)

---

### **ğŸ”¹ Use Case**
Used for classification problems, typically **binary (Yes/No, True/False) classification**.
- **Example**: Predicting whether a **customer will purchase a product** based on browsing behavior.

---

### **ğŸ”¹ Types of Logistic Regression**
1. **Binary Logistic Regression** â†’ Two possible outcomes (e.g., Spam vs. Not Spam).
2. **Multinomial Logistic Regression** â†’ Three or more unordered categories (e.g., choosing between multiple brands).
3. **Ordinal Logistic Regression** â†’ Three or more ordered categories (e.g., ranking customer satisfaction as Low, Medium, High).

---

### **ğŸ”¹ Special Property**
- Uses the **sigmoid function** to convert raw predictions into probability values.
- If \(p(Y=1) > 0.5\), classify as **Class 1**; else, classify as **Class 0**.

---

### **ğŸ”¹ Evaluation Metrics**
1. **Accuracy** â†’ Percentage of correct predictions.
2. **Precision** â†’ Correct **positive predictions** over all predicted positives.
3. **Recall** â†’ Correct **positive predictions** over all actual positives.
4. **F1-score** â†’ Harmonic mean of **precision and recall**.
5. **ROC Curve & AUC** â†’ Measures modelâ€™s performance in differentiating classes.

---

### **ğŸ”¹ Example**
**Predicting if a student will pass an exam**  
- **X (Independent Variable)** â†’ Hours studied  
- **Y (Dependent Variable)** â†’ Pass/Fail (1 or 0)  
- If the model predicts \(p(Y=1) = 0.85\), student is classified as **"Will Pass"**.

---

### **K-Nearest Neighbor (KNN)**
- **Works for both classification & regression tasks**.
- A **lazy algorithm** (doesnâ€™t build a model; instead, makes predictions using stored data points).

#### **ğŸ“Œ How It Works**
1. **Calculate distances** between test data and stored data points (Euclidean, Manhattan, Hamming).
Hereâ€™s the equation for **K-Nearest Neighbor (KNN)**:
   1. The distance between two points \( A(X_1, Y_1) \) and \( B(X_2, Y_2) \) is often computed using the **Euclidean Distance** formula:
   
   $$d = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}\$$

   Other distance metrics include:
   2. **Manhattan Distance**:
   
   $$d = |X_2 - X_1| + |Y_2 - Y_1|$$
   
   3. **Hamming Distance** (for categorical data):
   
   $$d = \sum (X_i \neq Y_i)$$

2. **Select K nearest points**:
   - **K â‰ˆ sqrt(n)** (where \(n\) is the number of data points).
   - **Odd K** is preferred to avoid ties between two classes.
3. **Determine category (classification) or mean value (regression).**

---

### **ğŸ”¹ Use Case**
Used in tasks where instances must be classified based on **similarity to known examples**.
- **Example**: Identifying **handwritten digits** by comparing with stored examples.

---

### **ğŸ”¹ Special Property**
- Doesnâ€™t assume **underlying data distribution**.
- Choosing **K** affects accuracy:  
  - **Small K** â†’ More variance, sensitive to noise.  
  - **Large K** â†’ More bias, less flexibility.

---

### **ğŸ”¹ Types of K-Nearest Neighbor **
1. **KNN for Classification** â†’ Assigns labels based on the majority of nearest neighbors.
   - Example: **Handwritten digit recognition**.
2. **KNN for Regression** â†’ Estimates a continuous value by averaging nearest neighbors.
   - Example: **Predicting temperature based on nearby weather stations**.
3. **Weighted KNN** â†’ Gives more influence to closer neighbors using weighted distance.
   - Example: **Personalized recommendation systems**.
4. **KNN for Clustering (unsupervised learning)** â†’ Groups similar data points together.
   - Example: **Customer segmentation in marketing**.

---
### **ğŸ”¹ Evaluation Metrics**
1. **Accuracy** â†’ Correct classifications.
2. **Confusion Matrix** â†’ Breakdown of correct and incorrect predictions.
3. **RMSE (if used for regression tasks)** â†’ Measures prediction error.
4. **Silhouette Score (for clustering)** â†’ Evaluates how well data points are grouped.

---

### **ğŸ”¹ Example**
**Classifying fruit types based on size and color**
- **Features (X):** Diameter, color intensity.
- **Labels (Y):** Apple, Orange, Banana.
- A new fruit is classified **based on the closest K samples** in the dataset.

---

## **1ï¸âƒ£ Decision Tree**
**ğŸ“Œ What It Is:**  
A hierarchical model that splits data into branches using decision rules based on feature values.

**ğŸ“Œ Equation:**  
The impurity measure for splitting nodes (Gini Index or Entropy) is calculated as:

- **Gini Index** (for classification):

$$Gini = 1 - \sum_{i=1}^{C} p_i^2$$

- **Entropy** (alternative impurity measure):

$$Entropy = - \sum_{i=1}^{C} p_i \log_2 p_i$$

Where \( p_i \) is the probability of class \( i \).

**ğŸ“Œ Use Case:**  
Used in **classification and regression tasks** where rule-based decisions work well.  
Example: **Fraud detection in banking**.

**ğŸ“Œ Types:**  
1. **Classification Trees** â†’ Predicts discrete labels (e.g., Yes/No).  
2. **Regression Trees** â†’ Predicts continuous values.  

**ğŸ“Œ Special Property:**  
Uses **pruning techniques** to prevent overfitting.

**ğŸ“Œ Evaluation Metrics:**  
- **Accuracy, Precision, Recall** (classification).  
- **Mean Squared Error (MSE)** (regression).  

**ğŸ“Œ Example:**  
Predicting **loan default risks** based on income, credit score, and loan amount.

---

## **2ï¸âƒ£ Random Forest**
**ğŸ“Œ What It Is:**  
An ensemble learning model that builds multiple decision trees and averages their predictions.

**ğŸ“Œ Equation:**  
For classification, final prediction is based on majority voting:

$$Prediction = \text{Mode}(\hat{y}_1, \hat{y}_2, ..., \hat{y}_n)$$

For regression, it is based on the mean value:

$$Prediction = \frac{1}{N} \sum_{i=1}^{N} \hat{y}_i$$

Where \( \hat{y}_i \) represents predictions from individual trees.

**ğŸ“Œ Use Case:**  
Used for **high-dimensional datasets** where individual decision trees may overfit.  
Example: **Predicting customer churn in telecom**.

**ğŸ“Œ Special Property:**  
Uses **Bootstrap Aggregating (Bagging)** to improve stability.

**ğŸ“Œ Evaluation Metrics:**  
- **Accuracy, ROC-AUC** for classification.  
- **MSE, RMSE** for regression.  

**ğŸ“Œ Example:**  
Predicting whether an **email is spam or not**, using multiple word features.

---

## **3ï¸âƒ£ Support Vector Machine (SVM)**
**ğŸ“Œ What It Is:**  
A model that finds the **optimal hyperplane** for separating classes.

**ğŸ“Œ Equation:**  
A hyperplane is defined as:

$$wX + b = 0$$

Where:
- \( w \) = Weight vector  
- \( X \) = Feature vector  
- \( b \) = Bias term

For classification, the goal is to **maximize the margin**:

$$\min \frac{1}{2} ||w||^2 \quad \text{subject to} \quad y_i(wX_i + b) \geq 1$$

**ğŸ“Œ Use Case:**  
Used in **text classification, bioinformatics, and image recognition**.  
Example: **Classifying cancer cells as benign or malignant**.

**ğŸ“Œ Types:**  
1. **Linear SVM** â†’ Best for **linearly separable data**.  
2. **Non-Linear SVM** â†’ Uses **Kernel Trick** for complex patterns.  

**ğŸ“Œ Special Property:**  
Maximizes **margin** to separate classes effectively.

**ğŸ“Œ Evaluation Metrics:**  
- **Precision, Recall, F1-score**.  
- **Hinge Loss** measures boundary correctness.

**ğŸ“Œ Example:**  
Classifying **customer reviews** as positive or negative.

---

## **4ï¸âƒ£ NaÃ¯ve Bayes**
**ğŸ“Œ What It Is:**  
A probabilistic model based on **Bayesâ€™ Theorem**.

**ğŸ“Œ Equation:**  
Bayesâ€™ Theorem is given by:

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

For classification:

$$P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)}$$

Where:
- \( P(A|B) \) = Posterior probability  
- \( P(B|A) \) = Likelihood  
- \( P(A) \) = Prior probability  
- \( P(B) \) = Evidence

**ğŸ“Œ Use Case:**  
Works well for **text classification, sentiment analysis, and spam filtering**.  
Example: **Detecting fake news articles based on text frequency**.

**ğŸ“Œ Types:**  
1. **Gaussian NaÃ¯ve Bayes** â†’ Assumes normal distribution for continuous data.  
2. **Multinomial NaÃ¯ve Bayes** â†’ Best for word frequency data.  
3. **Bernoulli NaÃ¯ve Bayes** â†’ Used for binary feature sets.

**ğŸ“Œ Special Property:**  
Assumes **independence between features** for fast computation.

**ğŸ“Œ Evaluation Metrics:**  
- **Log-Loss (Negative Log Likelihood)**.  
- **Accuracy, Precision, Recall**.

**ğŸ“Œ Example:**  
Classifying **news articles as real or fake** based on word occurrences.

---

## **5ï¸âƒ£ K-Means Clustering**
**ğŸ“Œ What It Is:**  
An unsupervised algorithm that groups similar data points into **K clusters**.

**ğŸ“Œ Equation:**  
The cluster centroids are updated iteratively:

$$C_k = \frac{1}{N} \sum_{i=1}^{N} X_i$$

Where:
- \( C_k \) = Centroid of cluster \( k \)  
- \( X_i \) = Data point assigned to cluster  
- \( N \) = Number of data points in the cluster

**ğŸ“Œ Use Case:**  
Used in **customer segmentation, anomaly detection, and recommendation systems**.  
Example: **Grouping customers based on spending habits**.

**ğŸ“Œ Special Property:**  
Uses **Euclidean Distance** for cluster assignment:

$$d = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}$$

**ğŸ“Œ Evaluation Metrics:**  
- **Silhouette Score** â†’ Measures cluster cohesion.  
- **WCSS (Within Cluster Sum of Squares)** â†’ Measures intra-cluster variance.

**ğŸ“Œ Example:**  
Segmenting **customers into high-spenders, mid-range, and budget shoppers**.

---

## **6ï¸âƒ£ Principal Component Analysis (PCA)**
**ğŸ“Œ What It Is:**  
A dimensionality reduction technique that removes redundancy while preserving variance.

**ğŸ“Œ Equation:**  
Eigenvalue decomposition of covariance matrix:

$$X' = W X$$

Where:
- \( W \) = Transformation matrix  
- \( X \) = Original dataset  
- \( X' \) = Reduced feature representation

**ğŸ“Œ Use Case:**  
Used for **reducing dataset size** while retaining variance.  
Example: **Facial recognition in security systems**.

**ğŸ“Œ Special Property:**  
- Converts correlated features into a set of **independent principal components**.  
- Retains **maximum variance** in fewer dimensions.

**ğŸ“Œ Evaluation Metrics:**  
- **Explained Variance Ratio** â†’ Indicates information retained.  
- **Reconstruction Error** â†’ Measures loss after compression.

**ğŸ“Œ Example:**  
Compressing **high-resolution images** into fewer dimensions.

---

## **7ï¸âƒ£ Neural Networks (Deep Learning)**
**ğŸ“Œ What It Is:**  
A powerful model that mimics the human brain using layers of interconnected neurons.

**ğŸ“Œ Equation:**  
A neural network's forward propagation is calculated as:

$$Z = W X + b$$

Activation function:

$$A = \sigma(Z)$$

Where:
- \( W \) = Weights  
- \( X \) = Input data  
- \( b \) = Bias  
- \( \sigma \) = Activation function (ReLU, Sigmoid, Softmax)

**ğŸ“Œ Use Case:**  
Used in **image recognition, speech processing, and language translation**.  
Example: **Predicting handwritten digits in OCR systems**.

**ğŸ“Œ Special Property:**  
Uses **backpropagation and gradient descent** for learning.

**ğŸ“Œ Evaluation Metrics:**  
- **Cross-Entropy Loss** (classification).  
- **MSE** (regression).  

**ğŸ“Œ Example:**  
Generating **realistic human faces** in AI-powered image synthesis.

---
